{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from gensim import models\n",
    "from gensim import matutils\n",
    "import logging\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from time import time\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import glob\n",
    "import argparse\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Finished\n"
     ]
    }
   ],
   "source": [
    "### Drap the review whcih has same business id in business object with 'Restaurants' categories.\n",
    "business_id = []\n",
    "business_output = []\n",
    "with open(\"yelp_academic_dataset.json\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        business_json = json.loads(line)\n",
    "        if 'categories' in business_json:\n",
    "            business_output.append(line)\n",
    "            if 'Restaurants' in business_json['categories']:\n",
    "                business_id.append(business_json['business_id'])\n",
    "print \"Business Finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Finished\n"
     ]
    }
   ],
   "source": [
    "review_output = []\n",
    "with open(\"yelp_academic_dataset.json\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        review_json = json.loads(line)\n",
    "        if 'text' in review_json:\n",
    "            if review_json['business_id'] in business_id:\n",
    "                review_output.append(line)\n",
    "with open ( \"yelp_academic_dataset_review.json\", 'w' ) as f:\n",
    "    f.write(''.join(review_output))\n",
    "print \"Review Finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving restaurant ratings\n",
      "finish process\n"
     ]
    }
   ],
   "source": [
    "path2buisness=\"yelp_academic_dataset_business.json\"\n",
    "path2reviews=\"yelp_academic_dataset_review.json\"\n",
    "\n",
    "categories = set([])\n",
    "restaurant_ids = set([])\n",
    "cat2rid = {}\n",
    "rest2rate={}\n",
    "rest2revID = {}\n",
    "r = 'Restaurants'\n",
    "with open (path2buisness, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        business_json = json.loads(line)\n",
    "        bjc = business_json['categories']\n",
    "        #cities.add(business_json['city'])\n",
    "        if r in bjc:\n",
    "            if len(bjc) > 1:\n",
    "                #print(bjc)\n",
    "                restaurant_ids.add(business_json['business_id'])\n",
    "                categories = set(bjc).union(categories) - set([r])\n",
    "                stars = business_json['stars']\n",
    "                rest2rate[ business_json['business_id'] ] = stars\n",
    "                for cat in bjc:\n",
    "                    if cat == r:\n",
    "                        continue\n",
    "                    if cat in cat2rid:\n",
    "                        cat2rid[cat].append(business_json['business_id'])\n",
    "                    else:\n",
    "                        cat2rid[cat] = [business_json['business_id']]\n",
    "\n",
    "print \"saving restaurant ratings\"\n",
    "with open ( 'restaurantIds2ratings.txt', 'w') as f:\n",
    "    for key in rest2rate:\n",
    "        f.write( key + \" \" + str(rest2rate[key]) + \"\\n\")\n",
    "    #clearing from memory\n",
    "rest2rate=None\n",
    "with open('data_cat2rid.pickle', 'wb') as f:\n",
    "    pickle.dump(cat2rid,f)\n",
    "\n",
    "with open (path2reviews, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        review_json = json.loads(line)\n",
    "        if review_json['business_id'] in restaurant_ids:\n",
    "            if review_json['business_id'] in rest2revID:\n",
    "                rest2revID[ review_json['business_id'] ].append(review_json['review_id'])\n",
    "            else:\n",
    "                rest2revID[ review_json['business_id'] ] = [ review_json['review_id'] ]\n",
    "\n",
    "with open('data_rest2revID.pickle', 'wb') as f:\n",
    "    pickle.dump(rest2revID,f)\n",
    "\n",
    "nz_count = 0\n",
    "valid_cats = []\n",
    "for i, cat in enumerate(cat2rid):\n",
    "    cat_total_reviews = 0\n",
    "    for rid in cat2rid[cat]:\n",
    "        #number of reviews for each of restaurants\n",
    "        if rid in rest2revID:\n",
    "            cat_total_reviews = cat_total_reviews + len(rest2revID[rid])\n",
    "\n",
    "    if cat_total_reviews > 30:\n",
    "        nz_count = nz_count + 1\n",
    "        valid_cats.append(cat)\n",
    "        #print( cat, cat_total_reviews)\n",
    "\n",
    "\n",
    "print \"finish process\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling categories\n",
      "reading from reviews file...\n",
      "finish reading reviews\n"
     ]
    }
   ],
   "source": [
    "sample_size = 10 #len(valid_cats) # This specifies how many cuisines you would like to save\n",
    "                                  # if this process takes too long you can change it to something smaller like 5, 6 ...\n",
    "    \n",
    "#print nz_count, ' non-zero number of reviews in categories out of', len(cat2rid), 'categories')\n",
    "#x = range(nz_count)\n",
    "###def sample_categories(sample_sizes = 10):\n",
    "print \"sampling categories\"\n",
    "sample_rid2cat={}\n",
    "\n",
    "cat_sample = random.sample(valid_cats, sample_size)\n",
    "for cat in cat_sample:\n",
    "    for rid in cat2rid[cat]:\n",
    "        if rid in rest2revID:\n",
    "            if rid not in sample_rid2cat:\n",
    "                sample_rid2cat[rid] = []\n",
    "            sample_rid2cat[rid].append(cat)\n",
    "#remove from memory\n",
    "rest2revID=None\n",
    "#    print (len(sample_rid2cat), len(cat2rid), len(valid_cats), len(cat_sample))\n",
    "    \n",
    "print \"reading from reviews file...\"\n",
    "#ensure categories is a directory\n",
    "sample_cat2reviews={}\n",
    "sample_cat2ratings={}\n",
    "num_reviews = 0\n",
    "with open (path2reviews, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        review_json = json.loads(line)\n",
    "        rid = review_json['business_id']\n",
    "        if rid in sample_rid2cat:\n",
    "            for rcat in sample_rid2cat [ rid ]:\n",
    "                num_reviews = num_reviews + 1\n",
    "                if rcat in sample_cat2reviews:\n",
    "                    sample_cat2reviews [ rcat ].append(review_json['text'])\n",
    "                    sample_cat2ratings [ rcat ].append( str(review_json['stars']) )\n",
    "                else:\n",
    "                    sample_cat2reviews [ rcat ] = [review_json['text']]\n",
    "                    sample_cat2ratings [ rcat ] = [ str(review_json['stars']) ]\n",
    "\n",
    "print \"finish reading reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving categories\n",
      "finish saving\n"
     ]
    }
   ],
   "source": [
    "print \"saving categories\"\n",
    "#save categories\n",
    "for cat in sample_cat2reviews:\n",
    "    with open ('categories/' + cat.replace('/', '-').replace(\" \", \"_\") + \".txt\" , 'w') as f:\n",
    "        f.write(u'\\n'.join(sample_cat2reviews[cat]).encode('utf-8').strip())\n",
    "\n",
    "print \"finish saving\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling restaurant reviews\n",
      "Bistros\n",
      "Cuban\n",
      "Cajun/Creole\n",
      "Egyptian\n",
      "Food\n",
      "Brasseries\n",
      "Cheesesteaks\n",
      "Asian Fusion\n",
      "Food Delivery Services\n",
      "Food Stands\n"
     ]
    }
   ],
   "source": [
    "#save sample for restaurant reviews\n",
    "print \"sampling restaurant reviews\"\n",
    "def sample_reviews(rate_sizes = 100000):\n",
    "    sample_size = min(rate_sizes, num_reviews)\n",
    "    rev_sample = random.sample(range(num_reviews), sample_size)\n",
    "    my_sample_v2 = []\n",
    "    sample_ratings = []\n",
    "    sorted_rev_sample = sorted(rev_sample)\n",
    "    count = 0\n",
    "    max_bound = 0\n",
    "    for cat in sample_cat2reviews:\n",
    "        print cat\n",
    "        new_max_bound = max_bound + len(sample_cat2reviews[cat])\n",
    "        while count < sample_size and sorted_rev_sample[count] < new_max_bound:\n",
    "            my_sample_v2.append( sample_cat2reviews[cat][ sorted_rev_sample[count] - max_bound ].replace(\"\\n\", \" \").strip() )\n",
    "            sample_ratings.append( sample_cat2ratings[cat][ sorted_rev_sample[count] - max_bound ] )\n",
    "            count = count + 1\n",
    "        max_bound = new_max_bound\n",
    "        #if count in rev_sample:\n",
    "        #    my_sample.append(rev.replace(\"\\n\", \" \").strip())\n",
    "        #count = count + 1\n",
    "\n",
    "    with open (\"review_sample_\" + str(rate_sizes) + \".txt\", 'w') as f:\n",
    "        f.write('\\n'.join(my_sample_v2).encode('ascii','ignore') )\n",
    "\n",
    "    with open (\"review_ratings_\" + str(rate_sizes) + \".txt\", 'w') as f:\n",
    "        f.write('\\n'.join(sample_ratings).encode('ascii','ignore') )\n",
    "        \n",
    "sample_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating cuisine matrix with: 10 cuisines\n",
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 4.575000s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_samples: 10, n_features: 10000\n"
     ]
    }
   ],
   "source": [
    "def sim_matrix():\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    K_clusters = 10\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n",
    "                                     min_df=2, stop_words='english',\n",
    "                                     use_idf=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    if not os.path.isdir(\"categories\"):\n",
    "        print \"you need to generate the cuisines files 'categories' folder first\"\n",
    "        return\n",
    "    \n",
    "    text = []\n",
    "    c_names = []\n",
    "    cat_list = glob.glob (\"categories/*\")\n",
    "    cat_size = len(cat_list)\n",
    "    if cat_size < 1:\n",
    "        print \"you need to generate the cuisines files 'categories' folder first\"\n",
    "        return\n",
    "    \n",
    "    sample_size = min(30, cat_size)\n",
    "    cat_sample = sorted( random.sample(range(cat_size), sample_size) )\n",
    "    #print (cat_sample)\n",
    "    count = 0\n",
    "    for i, item in enumerate(cat_list):\n",
    "        if i == cat_sample[count]:\n",
    "            li =  item.split('/')\n",
    "            cuisine_name = li[-1]\n",
    "            c_names.append(cuisine_name[:-4].replace(\"_\",\" \"))\n",
    "            with open ( item ) as f:\n",
    "                text.append(f.read().replace(\"\\n\", \" \"))\n",
    "            count = count + 1\n",
    "        \n",
    "        if count >= len(cat_sample):\n",
    "            print \"generating cuisine matrix with:\", count, \"cuisines\"\n",
    "            break\n",
    "\n",
    "    if len(text) < 1:\n",
    "        print \"the 'categories' folder does not contain any cuisines. Run this program ussing the '--cuisine' option\"\n",
    "    t0 = time()\n",
    "    print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "\n",
    "    corpus = matutils.Sparse2Corpus(X,  documents_columns=False)\n",
    "    lda = models.ldamodel.LdaModel(corpus, num_topics=100)\n",
    "\n",
    "    doc_topics = lda.get_document_topics(corpus)\n",
    "    cuisine_matrix = [] #similarity of topics\n",
    "    # computing cosine similarity matrix\n",
    "    for i, doc_a in enumerate(doc_topics):\n",
    "        #print (i)\n",
    "        sim_vecs = []\n",
    "        for j , doc_b in enumerate(doc_topics):\n",
    "            w_sum = 0\n",
    "            if ( i <= j ):\n",
    "                norm_a = 0\n",
    "                norm_b = 0\n",
    "                \n",
    "                for (my_topic_b, weight_b) in doc_b:\n",
    "                    norm_b = norm_b + weight_b*weight_b\n",
    "\n",
    "                for (my_topic_a, weight_a) in doc_a:\n",
    "                    norm_a = norm_a + weight_a*weight_a\n",
    "                    for (my_topic_b, weight_b) in doc_b:\n",
    "                        if ( my_topic_a == my_topic_b ):\n",
    "                            w_sum = w_sum + weight_a*weight_b\n",
    "\n",
    "                norm_a = math.sqrt(norm_a)\n",
    "                norm_b = math.sqrt(norm_b)\n",
    "                denom = (float) (norm_a * norm_b)\n",
    "                if denom < 0.0001:\n",
    "                    w_sum = 0\n",
    "                else:\n",
    "                    w_sum = w_sum/(denom)\n",
    "            else:\n",
    "                w_sum = cuisine_matrix[j][i]\n",
    "            sim_vecs.append(w_sum)\n",
    "\n",
    "        cuisine_matrix.append(sim_vecs)\n",
    "\n",
    "    with open( 'cuisine_sim_matrix.csv', 'w') as f:\n",
    "        for i_list in cuisine_matrix:\n",
    "            s = \"\"\n",
    "            my_max = max(i_list)\n",
    "            for tt in i_list:\n",
    "                s = s+str(tt/my_max) + \" \"\n",
    "            s = s.strip()\n",
    "            f.write(\",\".join(s.split())+\"\\n\") #should the list be converted to m\n",
    "\n",
    "    \n",
    "    with open('cuisine_indices.txt', 'w') as f:\n",
    "        f.write( \"\\n\".join(c_names))\n",
    "sim_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 4.983000s\n",
      "n_samples: 39874, n_features: 27638\n",
      "Applying topic modeling, using LDA\n",
      "100 topics\n",
      "done in 50.118000s\n",
      "writing topics to file: sample_topics.txt\n"
     ]
    }
   ],
   "source": [
    "def IdeaModel(K, numfeatures, sample_file, num_display_words, outputfile):\n",
    "    K_clusters = K\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=numfeatures,\n",
    "                                     min_df=2, stop_words='english',\n",
    "                                     use_idf=True)\n",
    "\n",
    "    text = []\n",
    "    with open (sample_file, 'r') as f:\n",
    "        text = f.readlines()\n",
    "\n",
    "    t0 = time()\n",
    "    print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "    \n",
    "    # mapping from feature id to acutal word\n",
    "    id2words ={}\n",
    "    for i,word in enumerate(vectorizer.get_feature_names()):\n",
    "        id2words[i] = word\n",
    "\n",
    "    t0 = time()\n",
    "    print(\"Applying topic modeling, using LDA\")\n",
    "    print(str(K_clusters) + \" topics\")\n",
    "    corpus = matutils.Sparse2Corpus(X,  documents_columns=False)\n",
    "    lda = models.ldamodel.LdaModel(corpus, num_topics=K_clusters, id2word=id2words)\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "        \n",
    "    output_text = []\n",
    "    for i, item in enumerate(lda.show_topics(num_topics=K_clusters, num_words=num_display_words, formatted=False)):\n",
    "        output_text.append(\"Topic: \" + str(i))\n",
    "        no_, terms = item\n",
    "        for term, weight in terms:\n",
    "            output_text.append( term + \" : \" + str(weight) )\n",
    "\n",
    "    print \"writing topics to file:\", outputfile\n",
    "    with open ( outputfile, 'w' ) as f:\n",
    "        f.write('\\n'.join(output_text))\n",
    "###default\n",
    "IdeaModel(int(100), int(50000), \"review_sample_100000.txt\", int(15), \"sample_topics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
